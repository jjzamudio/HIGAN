{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be used for calculating the power spectrum to be used in plotting for every epochs.\n",
    "\n",
    "* Sample a lot of 64x64x64 cubes from the real 2048x2048x2048 cube\n",
    "* Calculate power spectrum for each of the sample cubes\n",
    "* Calculate mean and stddev for each k\n",
    "* Save both as pickle to load later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python3/3.6.3/intel/lib/python3.6/site-packages/h5py-2.7.1-py3.6-linux-x86_64.egg/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import graphviz\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import h5py\n",
    "import timeit\n",
    "import time\n",
    "import json\n",
    "from scipy import stats\n",
    "import pickle as pkl\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run in Jupyter = True\n"
     ]
    }
   ],
   "source": [
    "run_in_jupyter = False\n",
    "try:\n",
    "    cfg = get_ipython().config \n",
    "    run_in_jupyter = True\n",
    "except:\n",
    "    run_in_jupyter = False\n",
    "    pass\n",
    "\n",
    "if run_in_jupyter:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "else: \n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "print(\"Run in Jupyter = \" + str(run_in_jupyter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT MOVE THESE TO THE UPPER CELL!!!!!\n",
    "import itertools\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import colors\n",
    "import h5py\n",
    "import matplotlib as mpl\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = \"3D try\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mode = \"continue\"                       # training OR continue = continue if load another model is loaded and continued to train\n",
    "continue_train_folder = \"jupyter-output-128\"    # \"jupyter-output-552\" # the folder where the previous run is for the saved model to be trained further\n",
    "netD_iter_file = \"netD_iter_16.pth\"         # netD_iter_xx.pth file that contains the state dict under models/\n",
    "netG_iter_file = \"netG_iter_16.pth\"         # netG_iter_xx.pth file that contains the state dict under models/\n",
    "\n",
    "batch_size = 16                       # BATCH_SIZE: batch size for training\n",
    "gpu_device = 0                              # GPU_DEVICE: gpu id (default 0)\n",
    "nc = 1                # NC: number of channels in images\n",
    "cube_size = 64       # for our dataset more like one edge of the subcube\n",
    "lr = 1e-4               # LR: learning rate - default: 5e-5 (rmsprop) , 1e-4:adam\n",
    "max_iter = 150         # MAX_ITER: max iteration for training\n",
    "\n",
    "optimizer_choice = \"adam\"     # adam or rmsprop\n",
    "adam_beta1 = 0.5     # default: 0.9    # coefficients used for computing running averages of gradient and its square \n",
    "adam_beta2 = 0.999     # default: 0.999\n",
    "lr_decay = False               # Square root decay-> Just True or False        ||||Enter False or if True -> a numeric value\n",
    "\n",
    "manual_seed = 1\n",
    "sample_size_multiplier = 101 * 6\n",
    "n_samples = batch_size * sample_size_multiplier      # on prince, number of samples to get from the training cube\n",
    "Diter_1 = 100   # default: 100\n",
    "Giter_1 = 1      # default: 1\n",
    "Diter_2 = 1      # default: 5\n",
    "Giter_2 = 1      # default: 1\n",
    "if run_mode == \"continue\":\n",
    "    gen_iterations_limit = 0  \n",
    "else:\n",
    "    gen_iterations_limit = 25 # default = 25\n",
    "edge_sample = cube_size\n",
    "edge_test = 512\n",
    "\n",
    "mmd2_D_train_limit = False       # if True, if MMD2_D is less than 0, the generator training will be skipped\n",
    "mmd2_D_skip_G_train = False\n",
    "\n",
    "enable_gradient_penalty = True  # True = GP is used\n",
    "lambda_gradpen = 1              # Juan = 10 | Demystifying MMD GANs = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert n_samples > Diter_1, \"The gen_iterations wont work properly!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice = \"conv\"                  # conv or con_fc\n",
    "dimension_choice = \"3D\"                # 2D or 3D\n",
    "nz = 128                           # hidden dimension channel\n",
    "\n",
    "reconstruction_loss = False            # enable reconstruction loss or not\n",
    "dist_ae = 'L2'                         # \"L2\" or \"L1\" -> Autoencoder reconstructruced cube loss choice,  \"cos\" doesnt work\n",
    "\n",
    "repulsive_loss = False                   # False | lambda_repulsive ex: {False, 0.1, 0.7, 1.0}\n",
    "bounded_rbf_kernel = True             # True or False\n",
    "\n",
    "mmd_kernel = \"rbf\"                     # rbf , rbf_ratio, poly , linear, rq , rq_linear\n",
    "sigma_list = [1]         # sigma for RBF Kernel MMD\n",
    "alpha_list = [2,5,10,20,40,80]         # alpha list for RQ Kernel MMD #[1, 2, 5, 10, 20, 40, 80]    # [0.2,0.5,1.0,2.0,5.0]\n",
    "\n",
    "\n",
    "weight_clip_enabled = False\n",
    "left_clamp =  -0.01                    # default: -0.01\n",
    "right_clamp = 0.01                     # default: 0.01\n",
    "\n",
    "encoder_py = \"\"\n",
    "decoder_py = \"\"\n",
    "\n",
    "model_param_init = \"normal\"    # normal OR xavier (doesn't work right now)\n",
    "\n",
    "\"\"\"\n",
    "The explanations can be found in Ratio Matching MMD Nets (2018) in \n",
    "Equation 3.\n",
    "\"\"\"\n",
    "lambda_MMD = 1.0   # not used anywhere\n",
    "lambda_AE_X = 8.0  # used in above calc only \n",
    "lambda_AE_Y = 8.0  # used in above calc only\n",
    "lambda_rg = 0.1 #16.0   # errD = torch.sqrt(mmd2_D) + lambda_rg * one_side_errD \\\n",
    "                            # also for errG too\n",
    "\n",
    "if not reconstruction_loss:\n",
    "    lambda_AE_X = 0.0  # used in above calc only \n",
    "    lambda_AE_Y = 0.0  # used in above calc only\n",
    "    \n",
    "\n",
    "min_var_est = 1e-30 # 1e-30, default:1e-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_in_jupyter:\n",
    "#     %run utils/power_spectrum_utils.py\n",
    "# else:\n",
    "from utils.power_spectrum_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_multiplier = 1e2                # the norm multiplier in the 3D visualization\n",
    "scatter_size_magnitude = False      # change scatter point radius based on the value of the point \n",
    "if run_in_jupyter:\n",
    "    plot_show_3d = True             # shows the 3d scatter plot\n",
    "    plot_show_other = True\n",
    "    plot_save_3d = True             # whether to save or not as png \n",
    "    plot_save_other = True\n",
    "else:\n",
    "    plot_show_3d = False            # shows the 3d scatter plot\n",
    "    plot_show_other = False\n",
    "    plot_save_3d = True             # whether to save or not as png \n",
    "    plot_save_other = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_in_jupyter:\n",
    "#     %run utils/logging_utils.py\n",
    "# else:\n",
    "from utils.logging_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./\"  # this goes to \n",
    "data_dir = \"../\"\n",
    "new_output_folder = get_output_folder(run_in_jupyter = run_in_jupyter)\n",
    "# new_output_folder = \"drive-output-XX\"   # for batch processing\n",
    "experiment = root_dir + \"outputs/\" + new_output_folder + \"/\"       # : output directory of saved models\n",
    "# print(experiment)\n",
    "\n",
    "model_save_folder = experiment + \"model/\"\n",
    "redshift_fig_folder = experiment + \"figures/\"        # folder to save mmd & related plots\n",
    "redshift_3dfig_folder = experiment + \"3d_figures/\"   # folder to save 3D plots\n",
    "testing_folder = experiment + \"testing/\"   # folder to save 3D plots\n",
    "\n",
    "save_model_every = 2               # (every x epoch) frequency to save the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = 1        # WORKERS: number of threads to load data\n",
    "redshift_info_folder = root_dir + \"redshift_info/\"   # save some info here as pickle to speed up processing\n",
    "redshift_raw_file = \"fields_z=5.0.hdf5\"\n",
    "# redshift_file = \"redshift0_4th_root.h5\"    # redshift cube to be used\n",
    "                                        # standardized_no_shift_redshift0.h5\n",
    "                                        # minmax_scale_01_redshift0.h5\n",
    "                                        # minmax_scale_neg11_redshift0.h5\n",
    "                                        # redshift0_4th_root.h5\n",
    "                                        # redshift0_6th_root.h5\n",
    "                                        # redshift0_8th_root.h5\n",
    "                                        # redshift0_16th_root.h5\n",
    "                                        # redshift0_4th_root_neg11.h5\n",
    "root = 8 # should be an integer\n",
    "inverse_transform = \"log_scale_neg11\"    # scale_01 / scale_neg11 / root / \n",
    "                                # root_scale_01 / root_scale_neg11\n",
    "                                # log_scale_01 / log_scale_neg11\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift Data Load\n",
    "\n",
    "Loading the raw data instead of the transformed data because the transformations are going to be done on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File used for analysis = ../fields_z=5.0.hdf5\n"
     ]
    }
   ],
   "source": [
    "# f = h5py.File(data_dir + redshift_file, 'r')\n",
    "f = h5py.File(data_dir + redshift_raw_file, 'r')\n",
    "print(\"File used for analysis = \" + str(f.filename))\n",
    "f = f['delta_HI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshift Info Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create redshift info folder if it doesn't exist\n",
    "if Path(redshift_info_folder).exists() == False:\n",
    "    os.mkdir(redshift_info_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if run_in_jupyter:\n",
    "#     %run utils/data_utils.py\n",
    "# else:\n",
    "from utils.data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed  Data Summary Statistics:\n",
      "Min of data = 0.0\n",
      "Max of data = 4376932000.0\n",
      "Mean of data = 14592.24\n",
      "Stddev of data = 922711.56\n",
      "\n",
      "Raw Data Summary Statistics:\n",
      "File = fields_z=5.0.hdf5\n",
      "Min of raw data = 0.0\n",
      "Max of raw data = 4376932000.0\n",
      "Mean of raw data = 14592.24\n",
      "Stddev of raw data = 922711.56\n"
     ]
    }
   ],
   "source": [
    "# min_cube,max_cube,mean_cube,stddev_cube = get_stats_cube(redshift_info_folder = redshift_info_folder,\n",
    "#                                            redshift_file = redshift_file,\n",
    "#                                            data_dir = data_dir)\n",
    "min_cube,max_cube,mean_cube,stddev_cube = get_stats_cube(redshift_info_folder = redshift_info_folder,\n",
    "                                           redshift_file = redshift_raw_file,\n",
    "                                           data_dir = data_dir)\n",
    "\n",
    "min_raw_cube,max_raw_cube,mean_raw_cube,stddev_raw_cube = get_stats_cube(redshift_info_folder = redshift_info_folder,\n",
    "                                           redshift_file = redshift_raw_file,\n",
    "                                           data_dir = data_dir)\n",
    "print(\"\\nTransformed  Data Summary Statistics:\")\n",
    "# print(\"File = \" + str(redshift_file))\n",
    "print(\"Min of data = \" + str(min_cube))\n",
    "print(\"Max of data = \" + str(max_cube))\n",
    "print(\"Mean of data = \" + str(mean_cube))\n",
    "print(\"Stddev of data = \" + str(stddev_cube))\n",
    "\n",
    "print(\"\\nRaw Data Summary Statistics:\")\n",
    "print(\"File = \" + str(redshift_raw_file))\n",
    "print(\"Min of raw data = \" + str(min_raw_cube))\n",
    "print(\"Max of raw data = \" + str(max_raw_cube))\n",
    "print(\"Mean of raw data = \" + str(mean_raw_cube))\n",
    "print(\"Stddev of raw data = \" + str(stddev_raw_cube))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_3d_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# on prince\n",
    "sampled_subcubes = HydrogenDataset(h5_file=redshift_raw_file,\n",
    "                                    root_dir = data_dir,\n",
    "                                    f = h5py.File(data_dir + redshift_raw_file, 'r')[\"delta_HI\"],\n",
    "                                    s_test = edge_test, \n",
    "                                    s_train = edge_sample,\n",
    "                                    s_sample = edge_sample, \n",
    "                                    nsamples = n_samples,\n",
    "                                   min_cube = min_cube,\n",
    "                                  max_cube = max_cube,\n",
    "                                  mean_cube = mean_cube,\n",
    "                                  stddev_cube = stddev_cube,\n",
    "                                   min_raw_cube = min_raw_cube,\n",
    "                                  max_raw_cube = max_raw_cube,\n",
    "                                  mean_raw_cube = mean_raw_cube,\n",
    "                                  stddev_raw_cube = stddev_raw_cube,\n",
    "                                  rotate_cubes = True,\n",
    "                                   transform = False,\n",
    "                                  root = root,\n",
    "                                  dimensions = dimension_choice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[6.5421e+00, 1.0711e+01, 3.1528e+00,  ..., 2.0225e+01,\n",
       "           3.9625e+01, 1.4963e+01],\n",
       "          [3.3278e+00, 6.1793e-01, 9.0984e+00,  ..., 1.3281e+01,\n",
       "           2.4598e+01, 1.7787e+01],\n",
       "          [3.4711e+00, 2.6696e+00, 3.0616e+00,  ..., 2.0637e+01,\n",
       "           1.0468e+01, 1.8329e+01],\n",
       "          ...,\n",
       "          [3.4446e+00, 1.3478e+01, 1.4383e+01,  ..., 2.6585e+01,\n",
       "           3.5941e+00, 5.8644e+00],\n",
       "          [1.8003e+01, 6.8536e+00, 2.2541e+01,  ..., 4.5425e+01,\n",
       "           5.0814e+00, 9.4452e+00],\n",
       "          [1.9891e+01, 7.5610e+00, 1.7696e+01,  ..., 3.2916e+01,\n",
       "           1.9948e+01, 1.9706e+01]],\n",
       "\n",
       "         [[4.0020e+00, 2.7466e+00, 2.7559e+00,  ..., 5.0444e+01,\n",
       "           6.5539e+01, 3.0063e+01],\n",
       "          [1.4743e-01, 2.1656e+00, 1.5861e+00,  ..., 4.2084e+01,\n",
       "           5.2978e+01, 2.6939e+01],\n",
       "          [3.5366e-01, 1.1314e+01, 5.7064e+00,  ..., 3.8384e+01,\n",
       "           3.6316e+01, 3.0298e+01],\n",
       "          ...,\n",
       "          [1.5088e+01, 2.0145e+01, 2.8516e+01,  ..., 1.1642e+01,\n",
       "           1.5973e+01, 6.2554e+00],\n",
       "          [3.8469e+01, 9.5943e+00, 5.2447e+01,  ..., 1.4469e+01,\n",
       "           7.0084e+00, 1.7207e+01],\n",
       "          [6.1834e+01, 1.7777e+01, 3.7278e+01,  ..., 1.9938e+01,\n",
       "           5.5196e+00, 6.7490e+00]],\n",
       "\n",
       "         [[3.3345e+00, 3.8126e+00, 1.4754e+01,  ..., 6.1739e+01,\n",
       "           7.6347e+01, 5.9757e+01],\n",
       "          [5.4542e+00, 7.6138e+00, 4.9950e+00,  ..., 5.9160e+01,\n",
       "           4.7758e+01, 5.1319e+01],\n",
       "          [9.4018e+00, 2.8019e+00, 9.3060e+00,  ..., 1.7022e+01,\n",
       "           4.4513e+01, 2.8785e+01],\n",
       "          ...,\n",
       "          [2.3970e+01, 5.6902e+01, 8.8086e+01,  ..., 9.7411e+00,\n",
       "           3.6483e+00, 1.2701e+00],\n",
       "          [8.5537e+01, 1.1901e+02, 8.2096e+01,  ..., 1.9732e+01,\n",
       "           7.3051e-01, 2.9368e-01],\n",
       "          [7.5171e+01, 8.2374e+01, 2.6034e+01,  ..., 2.7208e+01,\n",
       "           1.2220e+01, 1.4641e+01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[2.9064e+01, 6.6505e+01, 1.9460e+02,  ..., 3.3092e+01,\n",
       "           2.5628e+01, 1.3062e+01],\n",
       "          [1.9291e+00, 2.4876e+01, 3.1866e+02,  ..., 2.6053e+01,\n",
       "           1.6899e+01, 1.1398e+01],\n",
       "          [8.8258e-01, 1.6346e+01, 4.7978e+02,  ..., 1.7127e+01,\n",
       "           7.9140e+00, 4.0926e+00],\n",
       "          ...,\n",
       "          [1.4935e+01, 9.6678e+00, 2.3050e+01,  ..., 9.1153e+00,\n",
       "           9.1752e+00, 2.7011e+00],\n",
       "          [1.8960e+01, 2.1319e+01, 1.1570e+01,  ..., 4.4136e+00,\n",
       "           1.1489e+01, 2.2867e+01],\n",
       "          [2.3814e+01, 2.2117e+01, 1.5118e+01,  ..., 1.6220e+01,\n",
       "           3.2697e+01, 1.9440e+01]],\n",
       "\n",
       "         [[2.6279e+01, 1.7112e+01, 1.0981e+02,  ..., 4.2396e+01,\n",
       "           2.2426e+01, 1.8663e+01],\n",
       "          [7.7637e+00, 3.7593e+00, 6.3794e+01,  ..., 2.4693e+01,\n",
       "           2.0930e+01, 1.0022e+01],\n",
       "          [1.0901e+00, 5.3785e-01, 3.6120e+01,  ..., 1.1387e+01,\n",
       "           2.0191e+01, 1.4242e+01],\n",
       "          ...,\n",
       "          [9.0541e+00, 1.2220e+01, 1.3495e+01,  ..., 1.0643e+01,\n",
       "           6.5269e+00, 2.3241e+01],\n",
       "          [1.4602e+01, 2.9534e+01, 2.7636e+01,  ..., 1.3823e+01,\n",
       "           1.1284e+01, 4.1971e+00],\n",
       "          [2.4543e+01, 2.6998e+01, 2.0925e+01,  ..., 2.2164e+01,\n",
       "           1.1871e+01, 3.8991e+00]],\n",
       "\n",
       "         [[4.6705e+01, 3.6496e+01, 6.3911e+01,  ..., 4.5067e+01,\n",
       "           3.5854e+01, 3.1457e+01],\n",
       "          [1.4289e+01, 2.5393e+01, 5.9250e+01,  ..., 4.8165e+01,\n",
       "           4.6634e+01, 2.4535e+01],\n",
       "          [5.8695e-01, 8.2301e-01, 6.7971e+00,  ..., 4.2192e+01,\n",
       "           6.3388e+00, 2.0611e+01],\n",
       "          ...,\n",
       "          [7.2303e+00, 8.5014e+00, 2.7022e+01,  ..., 1.0266e+01,\n",
       "           1.0196e+01, 4.9715e+00],\n",
       "          [8.7507e+00, 1.2666e+01, 8.1266e+00,  ..., 1.4822e+01,\n",
       "           1.5531e+01, 9.2391e+00],\n",
       "          [1.2340e+01, 1.6878e+01, 1.8337e+01,  ..., 1.5949e+01,\n",
       "           1.4989e+01, 4.2326e+00]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_subcubes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n"
     ]
    }
   ],
   "source": [
    "Pks = []\n",
    "\n",
    "for i in range(5000):\n",
    "    if i % 100 == 0: print(i)\n",
    "    Pk_sample, k_sample = power_spectrum_np(cube = np.array(sampled_subcubes[0]), \n",
    "                                          mean_raw_cube = sampled_subcubes.mean_raw_val, \n",
    "                                          data_dim = \"3D\")\n",
    "#     print(\"Pk: {}\".format(Pk_sample))\n",
    "#     print(\"k: {}\".format(k_sample))\n",
    "#     print(len(Pk_sample))\n",
    "    Pks.append(Pk_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 31)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pks = np.array(Pks)\n",
    "Pks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "Pk_means = np.mean(a=Pks,axis=0)\n",
    "# Pk_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stddev\n",
    "Pk_std = np.std(a=Pks,axis=0)\n",
    "# Pk_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5th, 25th, 75th and 95th percentile\n",
    "Pk_min = np.percentile(a=Pks,q=0,axis=0)\n",
    "Pk_5th = np.percentile(a=Pks,q=5,axis=0)\n",
    "Pk_25th = np.percentile(a=Pks,q=25,axis=0)\n",
    "Pk_50th = np.percentile(a=Pks,q=50,axis=0)\n",
    "Pk_75th = np.percentile(a=Pks,q=75,axis=0)\n",
    "Pk_95th = np.percentile(a=Pks,q=95,axis=0)\n",
    "Pk_max = np.percentile(a=Pks,q=100,axis=0)\n",
    "\n",
    "# print(Pk_min)\n",
    "# print(Pk_5th)\n",
    "# print(Pk_25th)\n",
    "# print(Pk_50th)\n",
    "# print(Pk_75th)\n",
    "# print(Pk_95th)\n",
    "# print(Pk_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_spectrum_real = {\"Pk_means\":Pk_means,\n",
    "                       \"Pk_std\":Pk_std,\n",
    "                       \"Pk_min\":Pk_min,\n",
    "                       \"Pk_5th\":Pk_5th,\n",
    "                       \"Pk_25th\":Pk_25th,\n",
    "                       \"Pk_50th\":Pk_50th,\n",
    "                       \"Pk_75th\":Pk_75th,\n",
    "                       \"Pk_95th\":Pk_95th,\n",
    "                       \"Pk_max\":Pk_max}\n",
    "with open('power_spectrum/power_spectrum_real.pickle', 'wb') as handle:\n",
    "    pkl.dump(power_spectrum_real, handle, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
